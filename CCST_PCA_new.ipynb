{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dfc2c858",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xml.sax.handler import feature_validation\n",
    "from sklearn.metrics import (adjusted_rand_score, normalized_mutual_info_score, \n",
    "                             silhouette_score, calinski_harabasz_score,\n",
    "                             davies_bouldin_score)\n",
    "import logging\n",
    "from tqdm import tqdm\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from graphmae.utils import (\n",
    "    \n",
    "    build_args,\n",
    "    create_optimizer,\n",
    "    mask_edge,\n",
    "    set_random_seed,\n",
    "    TBLogger,\n",
    "    get_current_lr,\n",
    "    load_best_configs,\n",
    "\n",
    "    \n",
    ")\n",
    "from scanpy import read_10x_h5\n",
    "from collections import Counter\n",
    "from graphmae.datasets.data_util import load_dataset\n",
    "from graphmae.evaluation import node_classification_evaluation\n",
    "from graphmae.models import build_model\n",
    "from ogb.nodeproppred import DglNodePropPredDataset\n",
    "from sklearn.cluster import KMeans\n",
    "#import calculate_adj\n",
    "import anndata as ad\n",
    "import scanpy as sc\n",
    "from operator import index\n",
    "import re\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import sparse\n",
    "import dgl\n",
    "import torch\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "columns  = [\"dataset_name\",\"graph_devise\",\"threshold_num\",\"node_num\",\"edge_num\",\n",
    "            \"feature_dim\",\"feature_dim_num\",\"mask_rate\",\"encode_network\",\"decode_network\",\n",
    "            \"num_hidden\",\"num_layers\",\"activation\",\"max_epoch\",\"lr\",\"result\"]\n",
    "result_pd = pd.DataFrame(columns  = columns)\n",
    "class myDict(object):\n",
    "    def __init__(self,mydict):\n",
    "        self.mydict = mydict\n",
    "        self.length = []\n",
    "        self.keys = []\n",
    "        for key,values in self.mydict.items():\n",
    "            self.keys.append(key)\n",
    "            self.length.append(len(values))\n",
    "        self.nums = [1] * len(self.length)\n",
    "        for i in range(len(self.length)):\n",
    "            for j in range(i,len(self.length)):\n",
    "                self.nums[i] *= self.length[j]\n",
    "        self.para_dis = []\n",
    "        print(self.length)\n",
    "        print(self.nums)\n",
    "                \n",
    "    def getindex(self,index):\n",
    "        result = []\n",
    "        value = index\n",
    "        for i in range(len(self.nums) - 1):\n",
    "            result.append(value // self.nums[i+1])\n",
    "            value = value - result[i] * self.nums[i+1]\n",
    "        result.append(value) \n",
    "        result_dict = dict()\n",
    "        for index,value in enumerate(result):\n",
    "            result_dict[self.keys[index]] = self.mydict.get(self.keys[index])[value]\n",
    "        return result_dict\n",
    "    \n",
    "    #para_dis = []\n",
    "    def myiter(self):\n",
    "        #para_dis = []\n",
    "        for i in range(0,self.nums[0]):\n",
    "            self.para_dis.append(self.getindex(i))\n",
    "        return self.para_dis\n",
    "def kMeans_use(embedding,cluster_number):\n",
    "    kmeans = KMeans(n_clusters=cluster_number,\n",
    "                init=\"k-means++\",\n",
    "                random_state=0)\n",
    "    pred = kmeans.fit_predict(embedding)\n",
    "    return pred\n",
    "def pretrain(model, graph, feat, optimizer, max_epoch, device, scheduler, num_classes, lr_f, weight_decay_f, max_epoch_f, linear_prob, logger=None):\n",
    "    logging.info(\"start training..\")\n",
    "    graph = graph.to(device)\n",
    "    x = feat.to(device)\n",
    "\n",
    "    epoch_iter = tqdm(range(max_epoch))\n",
    "\n",
    "    for epoch in epoch_iter:\n",
    "        model.train()\n",
    "\n",
    "        loss, loss_dict = model(graph, x)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "\n",
    "        epoch_iter.set_description(f\"# Epoch {epoch}: train_loss: {loss.item():.4f}\")\n",
    "        if logger is not None:\n",
    "            loss_dict[\"lr\"] = get_current_lr(optimizer)\n",
    "            logger.note(loss_dict, step=epoch)\n",
    "\n",
    "        #if (epoch + 1) % 200 == 0:\n",
    "            #node_classification_evaluation(model, graph, x, num_classes, lr_f, weight_decay_f, max_epoch_f, device, linear_prob, mute=True)\n",
    "\n",
    "    # return best_model\n",
    "    return model\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description=\"GAT\")\n",
    "parser.add_argument(\"--seeds\", type=int, nargs=\"+\", default=[0])\n",
    "parser.add_argument(\"--dataset\", type=str, default=\"cora\")\n",
    "parser.add_argument(\"--device\", type=int, default=-1)\n",
    "parser.add_argument(\"--max_epoch\", type=int, default=200,\n",
    "                    help=\"number of training epochs\")\n",
    "parser.add_argument(\"--warmup_steps\", type=int, default=-1)\n",
    "\n",
    "parser.add_argument(\"--num_heads\", type=int, default=4,\n",
    "                    help=\"number of hidden attention heads\")\n",
    "parser.add_argument(\"--num_out_heads\", type=int, default=1,\n",
    "                    help=\"number of output attention heads\")\n",
    "parser.add_argument(\"--num_layers\", type=int, default=2,\n",
    "                    help=\"number of hidden layers\")\n",
    "parser.add_argument(\"--num_hidden\", type=int, default=256,\n",
    "                    help=\"number of hidden units\")\n",
    "parser.add_argument(\"--residual\", action=\"store_true\", default=False,\n",
    "                    help=\"use residual connection\")\n",
    "parser.add_argument(\"--in_drop\", type=float, default=.2,\n",
    "                    help=\"input feature dropout\")\n",
    "parser.add_argument(\"--attn_drop\", type=float, default=.1,\n",
    "                    help=\"attention dropout\")\n",
    "parser.add_argument(\"--norm\", type=str, default=None)\n",
    "parser.add_argument(\"--lr\", type=float, default=0.005,\n",
    "                    help=\"learning rate\")\n",
    "parser.add_argument(\"--weight_decay\", type=float, default=5e-4,\n",
    "                    help=\"weight decay\")\n",
    "parser.add_argument(\"--negative_slope\", type=float, default=0.2,\n",
    "                    help=\"the negative slope of leaky relu for GAT\")\n",
    "parser.add_argument(\"--activation\", type=str, default=\"prelu\")\n",
    "parser.add_argument(\"--mask_rate\", type=float, default=0.5)\n",
    "parser.add_argument(\"--drop_edge_rate\", type=float, default=0.0)\n",
    "parser.add_argument(\"--replace_rate\", type=float, default=0.0)\n",
    "\n",
    "parser.add_argument(\"--encoder\", type=str, default=\"gat\")\n",
    "parser.add_argument(\"--decoder\", type=str, default=\"gat\")\n",
    "parser.add_argument(\"--loss_fn\", type=str, default=\"byol\")\n",
    "parser.add_argument(\"--alpha_l\", type=float, default=2, help=\"`pow`inddex for `sce` loss\")\n",
    "parser.add_argument(\"--optimizer\", type=str, default=\"adam\")\n",
    "\n",
    "parser.add_argument(\"--max_epoch_f\", type=int, default=30)\n",
    "parser.add_argument(\"--lr_f\", type=float, default=0.001, help=\"learning rate for evaluation\")\n",
    "parser.add_argument(\"--weight_decay_f\", type=float, default=0.0, help=\"weight decay for evaluation\")\n",
    "parser.add_argument(\"--linear_prob\", action=\"store_true\", default=False)\n",
    "\n",
    "parser.add_argument(\"--load_model\", action=\"store_true\")\n",
    "parser.add_argument(\"--save_model\", action=\"store_true\")\n",
    "parser.add_argument(\"--use_cfg\", action=\"store_true\")\n",
    "parser.add_argument(\"--logging\", action=\"store_true\")\n",
    "parser.add_argument(\"--scheduler\", action=\"store_true\", default=False)\n",
    "parser.add_argument(\"--concat_hidden\", action=\"store_true\", default=False)\n",
    "\n",
    "# for graph classification\n",
    "parser.add_argument(\"--pooling\", type=str, default=\"mean\")\n",
    "parser.add_argument(\"--deg4feat\", action=\"store_true\", default=False, help=\"use node degree as input feature\")\n",
    "parser.add_argument(\"--batch_size\", type=int, default=32)\n",
    "columns  = [\"dataset_name\",\"graph_devise\",\"threshold_num\",\"node_num\",\"edge_num\",\n",
    "            \"feature_dim\",\"feature_dim_num\",\"mask_rate\",\"encode_network\",\"decode_network\",\n",
    "            \"num_hidden\",\"num_layers\",\"activation\",\"max_epoch\",\"lr\",\"result\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be59d9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "args = parser.parse_args([])\n",
    "args.lr = 0.001\n",
    "args.lr_f = 0.01\n",
    "args.num_hidden = 512\n",
    "args.num_heads = 4\n",
    "args.weight_decay = 2e-4\n",
    "args.weight_decay_f= 1e-4\n",
    "args.max_epoch= 600\n",
    "args.max_epoch_f= 300\n",
    "args.mask_rate= 0.5\n",
    "args.num_layers= 2\n",
    "args.encoder= \"gat\"\n",
    "args.decoder= \"gat\" \n",
    "args.activation= \"prelu\"\n",
    "args.in_drop= 0.2\n",
    "args.attn_drop= 0.1\n",
    "args.linear_prob= True\n",
    "args.loss_fn= \"sce\" \n",
    "args.drop_edge_rate=0.0\n",
    "args.optimizer= \"adam\"\n",
    "args.replace_rate= 0.05 \n",
    "args.alpha_l= 3\n",
    "args.scheduler= True\n",
    "args.dataset = \"sp\"\n",
    "arg.norm = \"graphnorm\"\n",
    "# \"layernorm\"\n",
    "# \"batchnorm\"\n",
    "# \"graphnorm\"\n",
    "\n",
    "#默认参数传递\n",
    "device = args.device if args.device >= 0 else \"cpu\"\n",
    "seeds = args.seeds\n",
    "dataset_name = args.dataset\n",
    "max_epoch = args.max_epoch\n",
    "max_epoch_f = args.max_epoch_f\n",
    "num_hidden = args.num_hidden\n",
    "num_layers = args.num_layers\n",
    "encoder_type = args.encoder\n",
    "decoder_type = args.decoder\n",
    "replace_rate = args.replace_rate\n",
    "\n",
    "optim_type = args.optimizer \n",
    "loss_fn = args.loss_fn\n",
    "\n",
    "lr = args.lr\n",
    "weight_decay = args.weight_decay\n",
    "lr_f = args.lr_f\n",
    "weight_decay_f = args.weight_decay_f\n",
    "linear_prob = args.linear_prob\n",
    "load_model = args.load_model\n",
    "save_model = args.save_model\n",
    "logs = args.logging\n",
    "use_scheduler = args.scheduler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "38491285",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunhang/anaconda3/envs/pytorch/lib/python3.7/site-packages/anndata/_core/anndata.py:1830: UserWarning: Variable names are not unique. To make them unique, call `.var_names_make_unique`.\n",
      "  utils.warn_names_duplicates(\"var\")\n"
     ]
    }
   ],
   "source": [
    "folder_name = \"/home/sunhang/Embedding/Spatial_dataset/DLPFC\"\n",
    "sample_name = str(151673)\n",
    "gene_loc_data_file = folder_name + \"/\" +sample_name+ \"/\" + sample_name + \"_DLPFC_col_name.csv\"\n",
    "adata_file = folder_name + \"/\" +sample_name+ \"/\" + sample_name + \"_filtered_feature_bc_matrix.h5\"\n",
    "gene_loc_data_csv = pd.read_csv(gene_loc_data_file,index_col=0)\n",
    "gene_loc_data_csv.index = gene_loc_data_csv.barcode\n",
    "gene_loc_data_csv = gene_loc_data_csv.fillna(\"None\")\n",
    "le = LabelEncoder()\n",
    "label = le.fit_transform(gene_loc_data_csv['layer_guess_reordered_short'])\n",
    "gene_loc_data_csv[\"lay_num\"] = label\n",
    "num_classes = len(set(gene_loc_data_csv.lay_num))\n",
    "if((gene_loc_data_csv['layer_guess_reordered_short'] == \"None\").any()):\n",
    "    num_classes = len(set(gene_loc_data_csv.lay_num)) - 1\n",
    "\n",
    "# Create a group with location informatio\n",
    "row_name = \"imagerow\"\n",
    "col_name = \"imagecol\"\n",
    "cell_loc = gene_loc_data_csv[[row_name,col_name]].values\n",
    "distance_np = pdist(cell_loc, metric = \"euclidean\")\n",
    "distance_np_X =squareform(distance_np)\n",
    "distance_loc_csv = pd.DataFrame(index=gene_loc_data_csv.index, columns=gene_loc_data_csv.index,data = distance_np_X)\n",
    "threshold = 8\n",
    "num_big = np.where((0< distance_np_X)&(distance_np_X < threshold))[0].shape[0]\n",
    "#num_big = np.where((0< distance_np_X)&(distance_np_X < threshold))[0].shape[0]\n",
    "adj_matrix = np.zeros(distance_np_X.shape)\n",
    "non_zero_point = np.where((0 < distance_np_X) & (distance_np_X < threshold))\n",
    "adj_matrix = np.zeros(distance_np_X.shape)\n",
    "non_zero_point = np.where((0< distance_np_X)&(distance_np_X<threshold))\n",
    "for i in range(num_big):\n",
    "    x = non_zero_point[0][i]\n",
    "    y = non_zero_point[1][i]\n",
    "    adj_matrix[x][y] = 1 \n",
    "adj_matrix = adj_matrix + np.eye(distance_np_X.shape[0])\n",
    "adj_matrix  = np.float32(adj_matrix)\n",
    "adj_matrix_crs = sparse.csr_matrix(adj_matrix)\n",
    "graph = dgl.from_scipy(adj_matrix_crs,eweight_name='w')\n",
    "\n",
    "\n",
    "adata = read_10x_h5(adata_file)\n",
    "adata.obs = pd.merge(adata.obs,gene_loc_data_csv,left_index=True,right_index=True)\n",
    "adata.var_names=[i.upper() for i in list(adata.var_names)]\n",
    "adata.var[\"genename\"]=adata.var.index.astype(\"str\")\n",
    "adata.var_names_make_unique\n",
    "pca_n_comps = 3000\n",
    "sc.pp.filter_genes(adata, min_cells=5)\n",
    "adata_X = sc.pp.normalize_total(adata, target_sum=1, exclude_highly_expressed=True, inplace=False)['X']\n",
    "adata_X = sc.pp.scale(adata_X)\n",
    "adata_X = sc.pp.pca(adata_X, n_comps=pca_n_comps)\n",
    "graph.ndata[\"feat\"] = torch.tensor(adata_X.copy())\n",
    "num_features = graph.ndata[\"feat\"].shape[1]\n",
    "args.num_features = num_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ac792069",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200\n",
      "0.0\n",
      "200\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5782/1143291568.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Embedding/myGraphMAE/graphmae/models/__init__.py\u001b[0m in \u001b[0;36mbuild_model\u001b[0;34m(args)\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0mreplace_rate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreplace_rate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m         \u001b[0malpha_l\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0malpha_l\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m         \u001b[0mconcat_hidden\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconcat_hidden\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m     )\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Embedding/myGraphMAE/graphmae/models/edcoder.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_dim, num_hidden, num_layers, nhead, nhead_out, activation, feat_drop, attn_drop, negative_slope, residual, norm, mask_rate, encoder_type, decoder_type, loss_fn, drop_edge_rate, replace_rate, alpha_l, concat_hidden)\u001b[0m\n\u001b[1;32m    151\u001b[0m             \u001b[0mnegative_slope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnegative_slope\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0mresidual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m             \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m         )\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Embedding/myGraphMAE/graphmae/models/edcoder.py\u001b[0m in \u001b[0;36msetup_module\u001b[0;34m(m_type, enc_dec, in_dim, num_hidden, out_dim, num_layers, dropout, activation, residual, norm, nhead, nhead_out, attn_drop, negative_slope, concat_out)\u001b[0m\n\u001b[1;32m     70\u001b[0m             \u001b[0mresidual\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresidual\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m             \u001b[0mnorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_norm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnorm\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc_dec\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"encoding\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         )\n\u001b[1;32m     74\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mm_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"mlp\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Embedding/myGraphMAE/graphmae/models/gcn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_dim, num_hidden, out_dim, num_layers, dropout, activation, residual, norm, encoding)\u001b[0m\n\u001b[1;32m     39\u001b[0m             \u001b[0;31m# input projection (no residual)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m             self.gcn_layers.append(GraphConv(\n\u001b[0;32m---> 41\u001b[0;31m                 in_dim, num_hidden, residual=residual, norm=norm, activation=create_activation(activation)))\n\u001b[0m\u001b[1;32m     42\u001b[0m             \u001b[0;31m# hidden layers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_layers\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Embedding/myGraphMAE/graphmae/models/gcn.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, in_dim, out_dim, norm, activation, residual)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnorm\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnorm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnorm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_activation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Embedding/myGraphMAE/graphmae/utils.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, hidden_dim, norm_type)\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean_scale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mParameter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mNotImplementedError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgraph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for num_set in np.arange(200,1000,100):\n",
    "    for num_pro_set in np.arange(0,1,0.1):\n",
    "        print(num_set)\n",
    "        print(num_pro_set)\n",
    "        folder_name = \"/home/sunhang/Embedding/Spatial_dataset/DLPFC\"\n",
    "        sample_name = str(151673)\n",
    "        gene_loc_data_file = folder_name + \"/\" +sample_name+ \"/\" + sample_name + \"_DLPFC_col_name.csv\"\n",
    "        adata_file = folder_name + \"/\" +sample_name+ \"/\" + sample_name + \"_filtered_feature_bc_matrix.h5\"\n",
    "        gene_loc_data_csv = pd.read_csv(gene_loc_data_file,index_col=0)\n",
    "        gene_loc_data_csv.index = gene_loc_data_csv.barcode\n",
    "        gene_loc_data_csv = gene_loc_data_csv.fillna(\"None\")\n",
    "        le = LabelEncoder()\n",
    "        label = le.fit_transform(gene_loc_data_csv['layer_guess_reordered_short'])\n",
    "        gene_loc_data_csv[\"lay_num\"] = label\n",
    "        num_classes = len(set(gene_loc_data_csv.lay_num))\n",
    "        if((gene_loc_data_csv['layer_guess_reordered_short'] == \"None\").any()):\n",
    "            num_classes = len(set(gene_loc_data_csv.lay_num)) - 1\n",
    "\n",
    "        # Create a group with location informatio\n",
    "        row_name = \"imagerow\"\n",
    "        col_name = \"imagecol\"\n",
    "        cell_loc = gene_loc_data_csv[[row_name,col_name]].values\n",
    "        distance_np = pdist(cell_loc, metric = \"euclidean\")\n",
    "        distance_np_X =squareform(distance_np)\n",
    "        distance_loc_csv = pd.DataFrame(index=gene_loc_data_csv.index, columns=gene_loc_data_csv.index,data = distance_np_X)\n",
    "        threshold = 8\n",
    "        num_big = np.where((0< distance_np_X)&(distance_np_X < threshold))[0].shape[0]\n",
    "        #num_big = np.where((0< distance_np_X)&(distance_np_X < threshold))[0].shape[0]\n",
    "        adj_matrix = np.zeros(distance_np_X.shape)\n",
    "        non_zero_point = np.where((0 < distance_np_X) & (distance_np_X < threshold))\n",
    "        adj_matrix = np.zeros(distance_np_X.shape)\n",
    "        non_zero_point = np.where((0< distance_np_X)&(distance_np_X<threshold))\n",
    "        for i in range(num_big):\n",
    "            x = non_zero_point[0][i]\n",
    "            y = non_zero_point[1][i]\n",
    "            adj_matrix[x][y] = 1 \n",
    "        adj_matrix = adj_matrix + np.eye(distance_np_X.shape[0])\n",
    "        adj_matrix  = np.float32(adj_matrix)\n",
    "        adj_matrix_crs = sparse.csr_matrix(adj_matrix)\n",
    "        graph = dgl.from_scipy(adj_matrix_crs,eweight_name='w')\n",
    "\n",
    "\n",
    "        adata = read_10x_h5(adata_file)\n",
    "        adata.obs = pd.merge(adata.obs,gene_loc_data_csv,left_index=True,right_index=True)\n",
    "        adata.var_names=[i.upper() for i in list(adata.var_names)]\n",
    "        adata.var[\"genename\"]=adata.var.index.astype(\"str\")\n",
    "        adata.var_names_make_unique\n",
    "        pca_n_comps = 3000\n",
    "        sc.pp.filter_genes(adata, min_cells=5)\n",
    "        adata_X = sc.pp.normalize_total(adata, target_sum=1, exclude_highly_expressed=True, inplace=False)['X']\n",
    "        adata_X = sc.pp.scale(adata_X)\n",
    "        adata_X = sc.pp.pca(adata_X, n_comps=pca_n_comps)\n",
    "        graph.ndata[\"feat\"] = torch.tensor(adata_X.copy())\n",
    "        num_features = graph.ndata[\"feat\"].shape[1]\n",
    "        args.num_features = num_features\n",
    "        graph.ndata[\"feat\"] = torch.tensor(adata_X.copy())\n",
    "        num_features = graph.ndata[\"feat\"].shape[1]\n",
    "        num_classes = len(set(adata.obs.lay_num))-1\n",
    "        #for num_set in np.arange(1000,4000,500):\n",
    "        print(num_set)\n",
    "        args = parser.parse_args([])\n",
    "        args.lr = 0.001\n",
    "        args.lr_f = 0.01\n",
    "        args.num_hidden = 512\n",
    "        args.num_heads = 4\n",
    "        args.weight_decay = 2e-4\n",
    "        args.weight_decay_f= 1e-4\n",
    "        args.max_epoch= 500\n",
    "        args.max_epoch_f= 500\n",
    "        args.mask_rate=  num_pro_set\n",
    "        args.num_layers= 2\n",
    "        args.encoder= \"gcn\"\n",
    "        args.decoder= \"gcn\" \n",
    "        args.activation= \"prelu\"\n",
    "        args.in_drop= 0.2\n",
    "        args.attn_drop= 0.1\n",
    "        args.linear_prob= True\n",
    "        args.loss_fn= \"sce\" \n",
    "        args.drop_edge_rate=0.0\n",
    "        args.optimizer= \"adam\"\n",
    "        args.replace_rate= 0.05 \n",
    "        args.alpha_l= 3\n",
    "        args.scheduler= True\n",
    "        args.dataset = \"sp\"\n",
    "        args.norm = \"graphnorm\"\n",
    "\n",
    "        #参数传递\n",
    "        device = args.device if args.device >= 0 else \"cpu\"\n",
    "        seeds = args.seeds\n",
    "        dataset_name = args.dataset\n",
    "        max_epoch = args.max_epoch\n",
    "        max_epoch_f = args.max_epoch_f\n",
    "        num_hidden = args.num_hidden\n",
    "        num_layers = args.num_layers\n",
    "        encoder_type = args.encoder\n",
    "        decoder_type = args.decoder\n",
    "        replace_rate = args.replace_rate\n",
    "\n",
    "        optim_type = args.optimizer \n",
    "        loss_fn = args.loss_fn\n",
    "\n",
    "        lr = args.lr\n",
    "        weight_decay = args.weight_decay\n",
    "        lr_f = args.lr_f\n",
    "        weight_decay_f = args.weight_decay_f\n",
    "        linear_prob = args.linear_prob\n",
    "        load_model = args.load_model\n",
    "        save_model = args.save_model\n",
    "        logs = args.logging\n",
    "        use_scheduler = args.scheduler\n",
    "        args.num_features = num_features\n",
    "\n",
    "        acc_list = []\n",
    "        estp_acc_list = []\n",
    "        times = 3\n",
    "\n",
    "        #print(f\"####### Run {i} for seed {seed}\")\n",
    "        #print(i)\n",
    "        seed = 0\n",
    "        set_random_seed(seed)\n",
    "        if logs:\n",
    "            logger = TBLogger(name=f\"{dataset_name}_loss_{loss_fn}_rpr_{replace_rate}_nh_{num_hidden}_nl_{num_layers}_lr_{lr}_mp_{max_epoch}_mpf_{max_epoch_f}_wd_{weight_decay}_wdf_{weight_decay_f}_{encoder_type}_{decoder_type}\")\n",
    "        else:\n",
    "            logger = None\n",
    "        model = build_model(args)\n",
    "        device = 1\n",
    "        model.to(device)\n",
    "        optimizer = create_optimizer(optim_type, model, lr, weight_decay)\n",
    "\n",
    "#         if use_scheduler:\n",
    "#             logging.info(\"Use schedular\")\n",
    "#             scheduler = lambda epoch :( 1 + np.cos((epoch) * np.pi / max_epoch) ) * 0.5\n",
    "#             # scheduler = lambda epoch: epoch / warmup_steps if epoch < warmup_steps \\\n",
    "#                     # else ( 1 + np.cos((epoch - warmup_steps) * np.pi / (max_epoch - warmup_steps))) * 0.5\n",
    "#             scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=scheduler)\n",
    "#         else:\n",
    "#             scheduler = None\n",
    "        #训练模型\n",
    "        x = graph.ndata[\"feat\"]\n",
    "        if not load_model:\n",
    "            model = pretrain(model, graph, x, optimizer, max_epoch, device, scheduler, num_classes, lr_f, weight_decay_f, max_epoch_f, linear_prob, logger)\n",
    "            #model = model.cpu()\n",
    "        x = graph.ndata[\"feat\"]\n",
    "        #model.to(device)\n",
    "        embedding = model.embed(graph.to(device), x.to(device))\n",
    "        new_pred = kMeans_use(embedding.cpu().detach().numpy(),num_classes)\n",
    "        adata.obs[\"pre\"] = new_pred\n",
    "        score = adjusted_rand_score(adata.obs.lay_num.values, new_pred )\n",
    "        print(\"first cluster:\")\n",
    "        print(score)\n",
    "        print(\"结果:\" + str(Counter(new_pred)))\n",
    "        print(\"ground_truth :\"  + str(Counter(adata.obs.lay_num.values)))\n",
    "        x = graph.ndata[\"feat\"]\n",
    "        test = model.embed(graph.to(device), x.to(device))\n",
    "        test_new_pred = kMeans_use(test.cpu().detach().numpy(),num_classes)\n",
    "        score = adjusted_rand_score(adata.obs.lay_num.values, test_new_pred )\n",
    "        adata.obs[\"second_pre\"] = test_new_pred\n",
    "        print(\"second cluster:\")\n",
    "        print(score)\n",
    "        print(\"结果:\" + str(Counter(test_new_pred)))\n",
    "        print(\"ground_truth :\"  + str(Counter(adata.obs.lay_num.values)))\n",
    "        #drawPicture(adata.obs,\"imagecol\",\"imagerow\",colorattribute=\"lay_num\",save_file= None)\n",
    "        #drawPicture(adata.obs,\"imagecol\",\"imagerow\",colorattribute=\"pre\",save_file= None)\n",
    "        #drawPicture(adata.obs,\"imagecol\",\"imagerow\",colorattribute=\"second_pre\",save_file= None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09fa67a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
